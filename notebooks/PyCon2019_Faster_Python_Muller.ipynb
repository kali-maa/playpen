{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Profiling\n",
    "\n",
    "# Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "repeat = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = timeit.timeit('a + b', globals=locals(), number=repeat) / repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_var = %timeit -o a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_var.average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lit = %timeit -o 1 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_var.average / t_lit.average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure whole cell without assignment, which is moved to pre-condition. Cannot have in-line comment in the cell before magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit a = 1; b = 2\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling\n",
    "\n",
    "Might want to automate this for your project.\n",
    "\n",
    "Note: `sleep` is inaccurate, especially on Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load measuring/profile_me.py\n",
    "# file profile_me.py\n",
    "\n",
    "\"\"\"Example to be profiled.\n",
    "\"\"\"\n",
    "import sys\n",
    "import time\n",
    "\n",
    "if sys.version_info.major < 3:\n",
    "    range = xrange\n",
    "\n",
    "\n",
    "def fast():\n",
    "    \"\"\"Wait 0.001 seconds.\n",
    "    \"\"\"\n",
    "    time.sleep(1e-3)\n",
    "\n",
    "\n",
    "def slow():\n",
    "    \"\"\"Wait 0.1 seconds.\n",
    "    \"\"\"\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "def use_fast():\n",
    "    \"\"\"Call `fast` 100 times.\n",
    "    \"\"\"\n",
    "    for _ in range(100):\n",
    "        fast()\n",
    "\n",
    "\n",
    "def use_slow():\n",
    "    \"\"\"Call `slow` 100 times.\n",
    "    \"\"\"\n",
    "    for _ in range(100):\n",
    "        slow()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    use_fast()\n",
    "    use_slow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = cProfile.Profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.runcall(use_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler2 = cProfile.Profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler2.runcall(use_slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler2.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('use_fast()', 'fast.stats')  # Store results in fast.stats file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pstats.Stats('fast.stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.print_callees('use_fast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_slow = %prun -s cumulative -r use_slow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_slow.print_stats(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wall Clock vs CPU Time\n",
    "\n",
    "For multiprocessing, wall clock time might be less than CPU time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load measuring/clock_check.py\n",
    "# file: measuring/clock_check.py\n",
    "\n",
    "\"\"\"Checking different timing functions.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "\n",
    "if sys.version_info.major < 3:\n",
    "    range = xrange\n",
    "\n",
    "\n",
    "def clock_check(duration=1):\n",
    "    \"\"\"Check the measured time with different methods.\n",
    "    \"\"\"\n",
    "    start_os_time0 = os.times()[0]  # CPU time\n",
    "    start_time_clock = time.clock()  # Depends on OS (CPU time on NIX, but not Windows)\n",
    "    start_default_timer = timeit.default_timer()  # Wall clock\n",
    "    for _ in range(int(1e6)):\n",
    "        1 + 1\n",
    "    time.sleep(duration)\n",
    "    durtation_os_time0 = os.times()[0] - start_os_time0\n",
    "    durtation_time_clock = time.clock() - start_time_clock\n",
    "    durtation_default_timer = timeit.default_timer() - start_default_timer\n",
    "    print('durtation_os_time0:     ', durtation_os_time0)\n",
    "    print('durtation_time_clock:   ', durtation_time_clock)\n",
    "    print('durtation_default_timer:', durtation_default_timer)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    clock_check()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note about deprecation warning: `time.perf_counter` is wall clock time, while `time.process_time` is CPU time. This takes the abstraction away from `time.clock`.\n",
    "\n",
    "If you pass in `time.process_time` into `cProfile.Profile`, you will no longer see wall clock time in the stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pi\n",
    "\n",
    "`simple_pi.py` is slower because it calculates one by one. `numpy_pi.py` is faster but it is memory-bound (sooner or later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load pi/simple_pi.py\n",
    "# file: simple_pi.py\n",
    "\n",
    "\"\"\"Calculating pi with Monte Carlo.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    range = xrange\n",
    "\n",
    "\n",
    "def pi_plain(total):\n",
    "    \"\"\"Calculate pi with `total` hits.\n",
    "    \"\"\"\n",
    "    count_inside = 0\n",
    "    for _ in range(total):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        dist = math.sqrt(x * x + y * y)\n",
    "        if dist < 1:\n",
    "            count_inside += 1\n",
    "    return 4.0 * count_inside / total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    def test():\n",
    "        \"\"\"Check if it works.\n",
    "        \"\"\"\n",
    "        n = int(1e6)\n",
    "        print('pi:', pi_plain(n))\n",
    "\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load pi/numpy_pi.py\n",
    "# file: numpy_pi.py\n",
    "\"\"\"Calculating pi with Monte Carlo Method and NumPy.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy                                                   #1\n",
    "\n",
    "\n",
    "def pi_numpy(total):                                           #2\n",
    "    \"\"\"Compute pi.\n",
    "    \"\"\"\n",
    "    x = numpy.random.rand(total)                               #3\n",
    "    y = numpy.random.rand(total)                               #4\n",
    "    dist = numpy.sqrt(x * x + y * y)                           #5\n",
    "    count_inside = len(dist[dist < 1])                         #6\n",
    "    return 4.0 * count_inside / total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    def test():\n",
    "        \"\"\"Time the execution.\n",
    "        \"\"\"\n",
    "        import timeit\n",
    "        start = timeit.default_timer()\n",
    "        pi_numpy(int(1e6))\n",
    "        print('run time', timeit.default_timer() - start)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%snakeviz pi_plain(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f use_fast use_fast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = list(range(1_000_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sys.getsizeof()` only measure object directly, not what is inside the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e6 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pympler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tracker.SummaryTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.print_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.print_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.print_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load measuring/memory_size_pympler.py\n",
    "# file: memory_size_pympler.py\n",
    "\n",
    "\"\"\"Measure the size of used memory with a decorator.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools                                                #1\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major < 3:\n",
    "    range = xrange\n",
    "\n",
    "from pympler import tracker                                     #2\n",
    "\n",
    "memory = {}                                                     #3\n",
    "\n",
    "\n",
    "def measure_memory(function):                                   #4\n",
    "    \"\"\"Decorator to measure memory size.\n",
    "    \"\"\"\n",
    "\n",
    "    @functools.wraps(function)                                  #5\n",
    "    def _measure_memory(*args, **kwargs):                       #6\n",
    "        \"\"\"This replaces the function that is to be measured.\n",
    "        \"\"\"\n",
    "        measurer = tracker.SummaryTracker()                     #7\n",
    "        for _ in range(2):                                      #8\n",
    "            measurer.diff()                                     #9\n",
    "        try:\n",
    "            res = function(*args, **kwargs)                     #10\n",
    "            return res\n",
    "        finally:                                                #11\n",
    "            memory[function.__name__] = (measurer.diff())\n",
    "    return _measure_memory                                      #12\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    @measure_memory                                             #13\n",
    "    def make_big(number):\n",
    "        \"\"\"Example function that makes a large list.\n",
    "        \"\"\"\n",
    "        return list(range(number))                              #14\n",
    "\n",
    "    make_big(int(1e6))                                          #15\n",
    "    print('used memory', memory)                                #16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getrefcount(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getrefcount(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load measuring/memory_growth_pympler.py\n",
    "# file memory_growth_pympler.py\n",
    "\n",
    "\"\"\"Measure the memory growth during a function call.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major < 3:\n",
    "    range = xrange\n",
    "\n",
    "from pympler import tracker                                     #1\n",
    "\n",
    "\n",
    "def check_memory_growth(function, *args, **kwargs):             #2\n",
    "    \"\"\"Measure the memory usage of `function`.\n",
    "    \"\"\"\n",
    "    measurer = tracker.SummaryTracker()                         #3\n",
    "    for _ in range(2):                                          #4\n",
    "        measurer.diff()                                         #5\n",
    "    function(*args, **kwargs)                                   #6\n",
    "    return measurer.diff()                                      #7\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    def test():\n",
    "        \"\"\"Do some tests with different memory usage patterns.\n",
    "        \"\"\"\n",
    "\n",
    "        def make_big(number):                                   #8\n",
    "            \"\"\"Function without side effects.\n",
    "\n",
    "            It cleans up all used memory after it returns.\n",
    "            \"\"\"\n",
    "            return list(range(number))\n",
    "\n",
    "        data = []                                               #9\n",
    "\n",
    "        def grow(number):\n",
    "            \"\"\"Function with side effects on global list.\n",
    "            \"\"\"\n",
    "            for x in range(number):\n",
    "                data.append(x)                                  #10\n",
    "        size = int(1e6)\n",
    "        print('memory make_big:', check_memory_growth(make_big,\n",
    "                                                      size))     #11\n",
    "        print('memory grow:', check_memory_growth(grow, size))   #12\n",
    "\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load measuring/pympler_list_growth.py\n",
    "# file: pympler_list_growth.py\n",
    "\n",
    "\"\"\"Measure the size of a list as it grows.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pympler.asizeof import asizeof, flatsize\n",
    "\n",
    "\n",
    "if sys.version_info.major < 3:\n",
    "    range = xrange\n",
    "\n",
    "\n",
    "def list_mem(length, size_func=flatsize):\n",
    "    \"\"\"Measure incremental memory increase of a growing list.\n",
    "    \"\"\"\n",
    "    my_list = []\n",
    "    mem = [size_func(my_list)]\n",
    "    for elem in range(length):\n",
    "        my_list.append(elem)\n",
    "        mem.append(size_func(my_list))\n",
    "    return mem\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    def main():\n",
    "        \"\"\"Show plot or numbers.\n",
    "        \"\"\"\n",
    "        SIZE = 1000\n",
    "        SHOW = 20\n",
    "\n",
    "        for func in [flatsize, asizeof, sys.getsizeof]:\n",
    "            mem = list_mem(SIZE, size_func=func)\n",
    "            try:\n",
    "                from matplotlib import pylab\n",
    "                pylab.plot(mem)\n",
    "                pylab.show()\n",
    "            except ImportError:\n",
    "                print('matplotlib seems not be installed. Skipping the plot.')\n",
    "                if SIZE > SHOW:\n",
    "                    limit = SHOW // 2\n",
    "                    print(mem[:limit],\n",
    "                          '... skipping %d elements ...' % (SIZE - SHOW),\n",
    "                          end='')\n",
    "                    print(mem[-limit:])\n",
    "                else:\n",
    "                    print(mem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flatsize` is the same as `sys.getsizeof`. For performance, `list` allocates memory in chunks because memory allocation is slow (I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load measuring/list_alloc_steps.py\n",
    "# file: list_alloc_steps.py\n",
    "\n",
    "\"\"\"Measure the number of memory allocation steps for a list.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major < 3:\n",
    "    range = xrange\n",
    "\n",
    "from pympler.asizeof import flatsize\n",
    "\n",
    "\n",
    "def list_steps(lenght, size_func=sys.getsizeof):\n",
    "    \"\"\"Measure the number of memory alloaction steps for a list.\n",
    "    \"\"\"\n",
    "    my_list = []\n",
    "    steps = 0\n",
    "    int_size = size_func(int())\n",
    "    old_size = size_func(my_list)\n",
    "    for elem in range(lenght):\n",
    "        my_list.append(elem)\n",
    "        new_size = sys.getsizeof(my_list)\n",
    "        if new_size - old_size > int_size:\n",
    "            steps += 1\n",
    "        old_size = new_size\n",
    "    return steps\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    steps = [10, 100, 1000, 10000, int(1e5), int(1e6), int(1e7)]\n",
    "    print('Using sys.getsizeof:')\n",
    "    for size in steps:\n",
    "        print('%10d: %3d' % (size, list_steps(size)))\n",
    "    print('Using pympler.asizeof.flatsize:')\n",
    "    for size in steps:\n",
    "        print('%10d: %3d' % (size, list_steps(size, flatsize)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"You can use the tools but take the results with a grain of salt.\"\n",
    "\n",
    "Note: Instructor skipped `memory_profiler` due to some \"compression\" stuff on Macs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation below is not a problem anymore in cPython but should be avoided in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ''\n",
    "\n",
    "for _ in range(n):\n",
    "    s += 'a'\n",
    "    \n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'a' * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = list(range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit res1 = [x + 10 for x in L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "res2 = []\n",
    "\n",
    "for x in L:\n",
    "    res2.append(x + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case above, list comprehension is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals vs Locals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithms/local_global.py\n",
    "# file: local_global.py\n",
    "\n",
    "\"\"\"Local vs. built-in.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major < 3:\n",
    "    range = xrange\n",
    "\n",
    "GLOBAL = 1\n",
    "\n",
    "\n",
    "def repeat(counter):\n",
    "    \"\"\"Using the GLOBAL value directly.\n",
    "    \"\"\"\n",
    "    for count in range(counter):\n",
    "        GLOBAL\n",
    "\n",
    "\n",
    "def repeat_local(counter):\n",
    "    \"\"\"Making GLOBAL a local variable.\n",
    "    \"\"\"\n",
    "    local = GLOBAL\n",
    "    for count in range(counter):\n",
    "        local\n",
    "\n",
    "\n",
    "def test(counter):\n",
    "    \"\"\"Call both functions.\n",
    "    \"\"\"\n",
    "    repeat(counter)\n",
    "    repeat_local(counter)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    def do_profile():\n",
    "        \"\"\"Check the run times.\n",
    "        \"\"\"\n",
    "        import cProfile\n",
    "        profiler = cProfile.Profile()\n",
    "        profiler.run('test(int(1e8))')\n",
    "        profiler.print_stats()\n",
    "\n",
    "    do_profile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, there is extra overhead in looking up global namespace. So, it is faster to assign it to local first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locals vs Built-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithms/local_builtin.py\n",
    "\"\"\"Local vs. built-in.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.major < 3:\n",
    "    range = xrange\n",
    "\n",
    "\n",
    "def repeat(counter):\n",
    "    \"\"\"Using the built-in `sum` in a loop.\n",
    "    \"\"\"\n",
    "    for count in range(counter):\n",
    "        sum\n",
    "\n",
    "\n",
    "def repeat_local(counter):\n",
    "    \"\"\"Making `sum` a local variable.\n",
    "    \"\"\"\n",
    "    sum_ = sum\n",
    "    for count in range(counter):\n",
    "        sum_\n",
    "\n",
    "\n",
    "def test(counter):\n",
    "    \"\"\"Call both functions.\n",
    "    \"\"\"\n",
    "    repeat(counter)\n",
    "    repeat_local(counter)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    def do_profile():\n",
    "        \"\"\"Check the run times.\n",
    "        \"\"\"\n",
    "        import cProfile\n",
    "        profiler = cProfile.Profile()\n",
    "        profiler.run('test(int(1e8))')\n",
    "        profiler.print_stats()\n",
    "\n",
    "    do_profile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like globals, there is overhead in looking up built-ins as opposed to assigning it to local first.\n",
    "\n",
    "There is some savings in using locals but not too much. What would really help is to use a different data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures\n",
    "\n",
    "## List vs Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load datastructure/searching.py\n",
    "# file: searching.py\n",
    "\"\"\"Measuring the time for searching in a list and a set.\n",
    "\"\"\"\n",
    "\n",
    "import timeit\n",
    "\n",
    "\n",
    "def search_list(n):\n",
    "    \"\"\"\n",
    "    Search for element that is not in a list.\n",
    "    \"\"\"\n",
    "    my_list = list(range(n))\n",
    "    start = timeit.default_timer()\n",
    "    n in my_list  # pylint: disable=pointless-statement\n",
    "    return timeit.default_timer() - start\n",
    "\n",
    "\n",
    "def search_set(n):\n",
    "    \"\"\"Search for an element in a set.\n",
    "    \"\"\"\n",
    "    my_set = set(range(n))\n",
    "    start = timeit.default_timer()\n",
    "    n in my_set  # pylint: disable=pointless-statement\n",
    "    return timeit.default_timer() - start\n",
    "\n",
    "\n",
    "def calculate_ratio(n):\n",
    "    \"\"\"Calculate the ratio between a search in a list and a set.\n",
    "    \"\"\"\n",
    "    list_time = search_list(n)\n",
    "    set_time = search_set(n)\n",
    "    return list_time, set_time, list_time / set_time\n",
    "\n",
    "\n",
    "def compare(end=8, func=calculate_ratio, header='', col1='List', col2='Set'):\n",
    "    \"\"\"Show the results.\n",
    "    \"\"\"\n",
    "    table_width = 43\n",
    "    print()\n",
    "    if header:\n",
    "        print('=' * table_width)\n",
    "        print(header)\n",
    "        print('=' * table_width)\n",
    "    width = end + end // 3\n",
    "    print('{:>{width}s} {:>9s} {:>9s} {:>12s}'.format(\n",
    "        'Size', col1, col2, 'Ratio', width=width))\n",
    "    print('-' * table_width)\n",
    "    fmt = '{count:{width},d} {list_time:9.2e} {set_time:9.2e} {ratio:12,.2f}'\n",
    "    for n in range(1, end):\n",
    "        count = 10 ** n\n",
    "        list_time, set_time, ratio = func(count)\n",
    "\n",
    "        print(fmt.format(count=count, ratio=ratio, list_time=list_time,\n",
    "                         set_time=set_time, width=width))\n",
    "    print('=' * table_width)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compare(header='Single run')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above shows that searching in list is slower than set because the latter uses a hash table. The advantage of converting list to set for lookup only makes sense if you are doing so many lookups that it becomes a bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(end=8, func=calculate_ratio, header='', col1='List', col2='Set'):\n",
    "    \"\"\"Show the results.\n",
    "    \"\"\"\n",
    "    table_width = 43\n",
    "    print()\n",
    "    if header:\n",
    "        print('=' * table_width)\n",
    "        print(header)\n",
    "        print('=' * table_width)\n",
    "    width = end + end // 3\n",
    "    print('{:>{width}s} {:>9s} {:>9s} {:>12s}'.format(\n",
    "        'Size', col1, col2, 'Ratio', width=width))\n",
    "    print('-' * table_width)\n",
    "    fmt = '{count:{width},d} {list_time:9.2e} {set_time:9.2e} {ratio:12,.2f}'\n",
    "    for n in range(1, end):\n",
    "        count = 10 ** n\n",
    "        list_time, set_time, ratio = func(count)\n",
    "\n",
    "        print(fmt.format(count=count, ratio=ratio, list_time=list_time,\n",
    "                         set_time=set_time, width=width))\n",
    "    print('=' * table_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load datastructure/searching_multiple.py\n",
    "# file: searching_multiple.py\n",
    "\"\"\"\n",
    "Measuring the time for searching in a list and a set multiple times.\n",
    "\"\"\"\n",
    "\n",
    "from statistics import mean\n",
    "import timeit\n",
    "\n",
    "\n",
    "def search_multiple(obj, n, repeat=7):\n",
    "    \"\"\"Search `repeat` times for at least 1 second.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for _ in range(repeat):\n",
    "        count = 0\n",
    "        duration = 0\n",
    "        while duration < 1:\n",
    "            start = timeit.default_timer()\n",
    "            n in obj  # pylint: disable=pointless-statement\n",
    "            duration += timeit.default_timer() - start\n",
    "            count += 1\n",
    "        res.append(duration / count)\n",
    "    return mean(res)\n",
    "\n",
    "\n",
    "def calculate_ratio_mutiple(n):\n",
    "    \"\"\"Calculate the ratio between a search in a list and a set.\n",
    "    \"\"\"\n",
    "    my_list = list(range(n))\n",
    "    my_set = set(range(n))\n",
    "    list_time = search_multiple(my_list, n)\n",
    "    set_time = search_multiple(my_set, n)\n",
    "    return list_time, set_time, list_time / set_time\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    compare(func=calculate_ratio_mutiple, header='Multiple runs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load datastructure/searching_magic.py\n",
    "# file: searching_magic.py\n",
    "\"\"\"\n",
    "Measuring the time for searching in a list and a set with IPythom %timeit.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.terminal.interactiveshell import TerminalInteractiveShell\n",
    "\n",
    "\n",
    "def timeit_magic(n, setup, statement):\n",
    "    \"\"\"Create a `%timeit` magic function with fixed `n`,\n",
    "    more setup code and the statement to be timed.\n",
    "    \"\"\"\n",
    "    return TerminalInteractiveShell().run_cell_magic(\n",
    "        'timeit', '-o -q n = {n}; '.format(n=n) + setup, statement)\n",
    "\n",
    "\n",
    "def search_list(n):\n",
    "    \"\"\"\n",
    "    Search for last element in a list.\n",
    "    \"\"\"\n",
    "    setup = 'my_list = list(range(n))'\n",
    "    statement = 'n in my_list'\n",
    "    return timeit_magic(n, setup, statement)\n",
    "\n",
    "\n",
    "def search_set(n):\n",
    "    \"\"\"Search for an element in a set.\n",
    "    \"\"\"\n",
    "    setup = 'my_set = set(range(n))'\n",
    "    statement = 'n in my_set'\n",
    "    return timeit_magic(n, setup, statement)\n",
    "\n",
    "\n",
    "def calculate_ratio(n, search_list=search_list, search_set=search_set):\n",
    "    \"\"\"Calculate the ratio between a search in a list and a set.\n",
    "    \"\"\"\n",
    "    list_time = search_list(n).average\n",
    "    set_time = search_set(n).average\n",
    "    return list_time, set_time, list_time / set_time\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compare(func=calculate_ratio, header='Magic timeit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load datastructure/intersect.py\n",
    "# file: intersect.py\n",
    "\"\"\"\n",
    "Measuring the time for searching in a list and a set including\n",
    "creation time of the data structure.\n",
    "\"\"\"\n",
    "\n",
    "import timeit\n",
    "\n",
    "\n",
    "def intersect_list(n):\n",
    "    \"\"\"Measure the run time for intersecting two lists.\n",
    "    \"\"\"\n",
    "    list_a = range(n)\n",
    "    list_b = range(n-3, 2 * n)\n",
    "    start = timeit.default_timer()\n",
    "    in_both = []\n",
    "    for x in list_a:\n",
    "        if x in list_b:\n",
    "            in_both.append(x)\n",
    "    run_time = timeit.default_timer() - start\n",
    "    return run_time, in_both\n",
    "\n",
    "\n",
    "def intersect_set(n):\n",
    "    \"\"\"Measure the run time for intersecting two setss.\n",
    "    \"\"\"\n",
    "    set_a = set(range(n))\n",
    "    set_b = set(range(n-3, 2 * n))\n",
    "    start = timeit.default_timer()\n",
    "    in_both = set_a.intersection(set_b)\n",
    "    run_time = timeit.default_timer() - start\n",
    "    return run_time, in_both\n",
    "\n",
    "\n",
    "def calculate_intersect(n):\n",
    "    \"\"\"Calculate the intersecting time for two lists and two sets.\n",
    "    \"\"\"\n",
    "    list_time, list_result = intersect_list(n)\n",
    "    set_time, set_result = intersect_set(n)\n",
    "    assert set_result == set(list_result)\n",
    "    return list_time, set_time, list_time / set_time\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    compare(func=calculate_intersect, header='Intersection')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deque\n",
    "\n",
    "List is efficient if you append, but not if you insert into the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L[2:4] = []  # Remove elements from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = deque(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.rotate(-4)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.pop()\n",
    "d.pop()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.rotate(2)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load datastructure/list_deque.py\n",
    "# file: list_deque.py\n",
    "\n",
    "\"\"\"Removing elements from a list vs. from a deque.\n",
    "\"\"\"\n",
    "\n",
    "from collections import deque\n",
    "from statistics import mean\n",
    "import timeit\n",
    "\n",
    "\n",
    "def time_function(func, make_args, repeat=7, limit=1):\n",
    "    \"\"\"Measure the run time of a function.\"\"\"\n",
    "    timing_res = []\n",
    "    for _ in range(repeat):\n",
    "        count = 0\n",
    "        duration = 0\n",
    "        while duration < limit:\n",
    "            args = make_args()\n",
    "            start = timeit.default_timer()\n",
    "            func(*args)\n",
    "            duration += timeit.default_timer() - start\n",
    "            count += 1\n",
    "        timing_res.append(duration / count)\n",
    "    return mean(timing_res)\n",
    "\n",
    "\n",
    "def remove_from_list(my_list, start, end):\n",
    "    \"\"\"Remove elements between `start` and `end` from a list.\n",
    "    \"\"\"\n",
    "    my_list[start:end] = []\n",
    "\n",
    "\n",
    "def remove_from_deque(my_deque, start, end):\n",
    "    \"\"\"Remove elements between `start` and `end` from a deque.\n",
    "    \"\"\"\n",
    "    my_deque.rotate(-end)\n",
    "    for _ in range(end - start):\n",
    "        my_deque.pop()\n",
    "    my_deque.rotate(start)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run some tests.\n",
    "    \"\"\"\n",
    "    start = 100\n",
    "    size = int(1e6)\n",
    "    fmt = '{diff:10,d} {list_time:10.2e} {deque_time: 10.2e} {ratio:8.2f}'\n",
    "    for limit in [0.00001, 0.0001, 0.001]:  #, 0.01, 0.1]:  # Took very long\n",
    "        print('Limit:', limit)\n",
    "        print('{:>10s} {:>10s} {:>10s} {:>8s}'.format(\n",
    "            'Replaced', 'List', 'Deque', 'Ratio'))\n",
    "        for end in [101, 110, 1100, 10100, 100100]:\n",
    "            diff = end - start\n",
    "            results = {}\n",
    "            for obj, func in zip([list, deque], [remove_from_list,\n",
    "                                                 remove_from_deque]):\n",
    "                def make_args(obj=obj, size=size, start=start, end=end):\n",
    "                    \"\"\"Dynamically create function with right arguments.\n",
    "                    \"\"\"\n",
    "                    return obj(range(size)), start, end\n",
    "\n",
    "                res = time_function(func, make_args, limit=limit)\n",
    "                results[obj.__name__] = res\n",
    "            list_time = results['list']\n",
    "            deque_time = results['deque']\n",
    "            ratio = list_time / deque_time\n",
    "            print(fmt.format(diff=diff, list_time=list_time,\n",
    "                             deque_time=deque_time, ratio=ratio))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above shows that algorithm optimization also depends on your data. Do you have big or small arrays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'adasdasdsadsad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "for k in s:\n",
    "    d.setdefault(k, 0)\n",
    "    d[k] += 1\n",
    "    \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d2 = defaultdict(int)  # Has to pass in something callable\n",
    "\n",
    "for k in s:\n",
    "    d2[k] += 1\n",
    "    \n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load datastructure/setdefault_defaultdict.py\n",
    "# file: setdefault_defaultdict.py\n",
    "\n",
    "\"\"\"Defaultdict can faster than a standard dict.\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def standard_dict(text):\n",
    "    \"\"\"Count with standard dict.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for key in text:\n",
    "        d.setdefault(key, 0)\n",
    "        d[key] += 1\n",
    "    return d\n",
    "\n",
    "\n",
    "def default_dict(text):\n",
    "    \"\"\"Count with defaultdict.\n",
    "    \"\"\"\n",
    "    dd = defaultdict(int)\n",
    "    for key in text:\n",
    "        dd[key] += 1\n",
    "    return dd\n",
    "\n",
    "\n",
    "def standard_dict_group(data):\n",
    "    \"\"\"Group with standard dict.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for key, value in data:\n",
    "        d.setdefault(key, []).append(value)\n",
    "    return d\n",
    "\n",
    "\n",
    "def default_dict_group(data):\n",
    "    \"\"\"Group with defaultdict.\n",
    "    \"\"\"\n",
    "    dd = defaultdict(list)\n",
    "    for key, value in data:\n",
    "        dd[key].append(value)\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit standard_dict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit default_dict(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, `defaultdict` is a bit faster but not by much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`O(1)` is independent of data size. `O(n)` is linear to size. Then, there is quadratic, and so on. It is good to understand big-O notation of your algorithms. Measure, measure, measure!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
